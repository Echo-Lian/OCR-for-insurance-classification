This project is from datacamp.com which is designed to use multi-modal learning to train an Optical Character Recognition (OCR) model with multimodal data. To improve the classification, the model will use images of the scanned documents as input and users' insurance type (home, life, auto, health, or other). Integrating different data modalities (such as image and text) enables the model to perform better in complex scenarios, helping to capture more nuanced information. The labels that the model will be trained to identify are of two types: a primary and a secondary ID, for each image-insurance type pair.
